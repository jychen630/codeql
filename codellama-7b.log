nohup: ignoring input

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.3 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/scratch/jc9723/codeql/sec_eval.py", line 12, in <module>
    from safecoder.utils import set_logging, set_seed, get_cp_args
  File "/scratch/jc9723/codeql/safecoder/utils.py", line 4, in <module>
    import torch
  File "/scratch/jc9723/miniconda3/lib/python3.12/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/scratch/jc9723/miniconda3/lib/python3.12/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/scratch/jc9723/miniconda3/lib/python3.12/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/scratch/jc9723/miniconda3/lib/python3.12/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/scratch/jc9723/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/scratch/jc9723/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
11/12/2024 09:36:19 - INFO - root -   args: Namespace(output_name='codellama-7b', model_name='codellama-7b', eval_type='trained-new', sec_prompting='none', vul_type=None, num_samples=100, num_samples_per_gen=20, temp=0.4, max_gen_len=256, top_p=0.95, experiments_dir='experiments/sec_eval', data_dir='data_eval/sec_eval/trained-new', model_dir='../trained', seed=1, output_dir='experiments/sec_eval/codellama-7b/trained-new', logger=<RootLogger root (INFO)>)
11/12/2024 09:36:20 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00,  9.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.65s/it]
experiments/sec_eval/codellama-7b/trained-new/cwe-022 <- output_dir
data_eval/sec_eval/trained-new/cwe-022 <- data_dir
Evaluating cwe-022:   0%|          | 0/3 [00:00<?, ?it/s]